{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e4f51e",
   "metadata": {},
   "source": [
    "# LLM for predicting the Warehouse demand to optimize the Inventory using GPT-2 model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33dd6800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 0: Importing required library packages \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e72d1837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "data = pd.read_csv(\"Warehouse_Prediction_Dataset.csv\") \n",
    "data_dict = {\n",
    "    \"query\": data[\"Query\"].tolist(),\n",
    "    \"response\": data[\"Response\"].tolist()\n",
    "}\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "hf_dataset = Dataset.from_dict(data_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6358257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Response</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the current stock level of Product A?</td>\n",
       "      <td>The current stock level of Product A is 500 un...</td>\n",
       "      <td>Product A is a high-demand item.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When should we restock Product B?</td>\n",
       "      <td>Product B should be restocked in 2 weeks.</td>\n",
       "      <td>Product B has a lead time of 3 weeks, so plan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the forecasted demand for Product C ne...</td>\n",
       "      <td>The forecasted demand for Product C next month...</td>\n",
       "      <td>Product C is affected by seasonal demand fluct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many units of Product D were sold last week?</td>\n",
       "      <td>Last week, 300 units of Product D were sold.</td>\n",
       "      <td>Product D sales are consistent week over week.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the lead time for Product E?</td>\n",
       "      <td>The lead time for Product E is 5 days.</td>\n",
       "      <td>Product E is sourced locally, ensuring quick d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Are there any seasonal trends for Product F?</td>\n",
       "      <td>Product F has higher sales during the holiday ...</td>\n",
       "      <td>Product F peaks in December due to holiday dem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the recommended reorder quantity for P...</td>\n",
       "      <td>The recommended reorder quantity for Product G...</td>\n",
       "      <td>Product G has a bulk discount when reordered i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the storage capacity utilization in th...</td>\n",
       "      <td>The warehouse is currently at 75% storage capa...</td>\n",
       "      <td>Storage optimization is being monitored for ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Which products are at risk of stockout?</td>\n",
       "      <td>Products X and Y are at risk of stockout based...</td>\n",
       "      <td>Stockout risk is calculated based on current s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the average daily sales of Product H?</td>\n",
       "      <td>The average daily sales of Product H are 50 un...</td>\n",
       "      <td>Sales data for Product H shows steady growth.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query  \\\n",
       "0      What is the current stock level of Product A?   \n",
       "1                  When should we restock Product B?   \n",
       "2  What is the forecasted demand for Product C ne...   \n",
       "3   How many units of Product D were sold last week?   \n",
       "4               What is the lead time for Product E?   \n",
       "5       Are there any seasonal trends for Product F?   \n",
       "6  What is the recommended reorder quantity for P...   \n",
       "7  What is the storage capacity utilization in th...   \n",
       "8            Which products are at risk of stockout?   \n",
       "9      What is the average daily sales of Product H?   \n",
       "\n",
       "                                            Response  \\\n",
       "0  The current stock level of Product A is 500 un...   \n",
       "1          Product B should be restocked in 2 weeks.   \n",
       "2  The forecasted demand for Product C next month...   \n",
       "3       Last week, 300 units of Product D were sold.   \n",
       "4             The lead time for Product E is 5 days.   \n",
       "5  Product F has higher sales during the holiday ...   \n",
       "6  The recommended reorder quantity for Product G...   \n",
       "7  The warehouse is currently at 75% storage capa...   \n",
       "8  Products X and Y are at risk of stockout based...   \n",
       "9  The average daily sales of Product H are 50 un...   \n",
       "\n",
       "                                             Context  \n",
       "0                   Product A is a high-demand item.  \n",
       "1  Product B has a lead time of 3 weeks, so plan ...  \n",
       "2  Product C is affected by seasonal demand fluct...  \n",
       "3     Product D sales are consistent week over week.  \n",
       "4  Product E is sourced locally, ensuring quick d...  \n",
       "5  Product F peaks in December due to holiday dem...  \n",
       "6  Product G has a bulk discount when reordered i...  \n",
       "7  Storage optimization is being monitored for ef...  \n",
       "8  Stockout risk is calculated based on current s...  \n",
       "9      Sales data for Product H shows steady growth.  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ce01eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split the dataset using Hugging Face's method\n",
    "train_test = hf_dataset.train_test_split(test_size=0.2) \n",
    "train_dataset = train_test['train']\n",
    "eval_dataset = train_test['test']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "135046c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AjayG\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at C:\\Users\\AjayG/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\AjayG/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\AjayG/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\merges.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\AjayG/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\AjayG/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\AjayG/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1898eace45ce4d9ab83dcd775eb2e70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd30c02d1bf54ebdaaf64234325115a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3: Tokenize the data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"query\"],\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"response\"],\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Applying tokenizer and preparing dataset\n",
    "encoded_train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"query\", \"response\"])\n",
    "encoded_eval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=[\"query\", \"response\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e53f3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\AjayG/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\AjayG/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\pytorch_model.bin\n",
      "C:\\Users\\AjayG\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Define the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\") # Model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bf2c748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "C:\\Users\\AjayG\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6\n",
      "  Number of trainable parameters = 124439808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>8.306581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.556075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.836059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./warehouse_model\\checkpoint-2\n",
      "Configuration saved in ./warehouse_model\\checkpoint-2\\config.json\n",
      "Model weights saved in ./warehouse_model\\checkpoint-2\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./warehouse_model\\checkpoint-4\n",
      "Configuration saved in ./warehouse_model\\checkpoint-4\\config.json\n",
      "Model weights saved in ./warehouse_model\\checkpoint-4\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./warehouse_model\\checkpoint-6\n",
      "Configuration saved in ./warehouse_model\\checkpoint-6\\config.json\n",
      "Model weights saved in ./warehouse_model\\checkpoint-6\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=7.3150787353515625, metrics={'train_runtime': 49.3135, 'train_samples_per_second': 0.487, 'train_steps_per_second': 0.122, 'total_flos': 1567752192000.0, 'train_loss': 7.3150787353515625, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Fine-tune the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./warehouse_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "284cc078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./warehouse_llm\\config.json\n",
      "Model weights saved in ./warehouse_llm\\pytorch_model.bin\n",
      "tokenizer config file saved in ./warehouse_llm\\tokenizer_config.json\n",
      "Special tokens file saved in ./warehouse_llm\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./warehouse_llm\\\\tokenizer_config.json',\n",
       " './warehouse_llm\\\\special_tokens_map.json',\n",
       " './warehouse_llm\\\\vocab.json',\n",
       " './warehouse_llm\\\\merges.txt',\n",
       " './warehouse_llm\\\\added_tokens.json',\n",
       " './warehouse_llm\\\\tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Save the fine-tuned model\n",
    "model.save_pretrained(\"./warehouse_llm\")\n",
    "tokenizer.save_pretrained(\"./warehouse_llm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8521088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Inference Function with Numerical Prediction Capability\n",
    "def get_response(query):\n",
    "    # Check if query is a number for direct prediction\n",
    "    try:\n",
    "        feature_value = float(query)\n",
    "        # Here you would implement your regression logic to predict demand based on feature_value.\n",
    "        predicted_demand = 10 + (feature_value * 5)  # Example linear relationship\n",
    "        return f\"The predicted demand for a feature value of {feature_value} is {predicted_demand:.2f}.\"\n",
    "    \n",
    "    except ValueError:\n",
    "        # If it's not a number, use the language model to generate a response.\n",
    "        inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        outputs = model.generate(**inputs, max_length=128, num_return_sequences=1)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return response\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35ca2339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Warehouse Demand Prediction Chatbot!\n",
      "Type 'exit' to quit.\n",
      "You: 8.693\n",
      "Bot: The predicted demand for a feature value of 8.693 is 53.46.\n",
      "You: exit\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Chatbot Interface for Interaction\n",
    "def chatbot_interface():\n",
    "    print(\"Welcome to the Warehouse Demand Prediction Chatbot!\")\n",
    "    print(\"Type 'exit' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        \n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        response = get_response(user_input)\n",
    "        print(\"Bot:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot_interface()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d183f645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
